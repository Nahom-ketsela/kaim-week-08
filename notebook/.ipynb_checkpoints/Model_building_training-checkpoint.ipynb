{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40af52c2-0efd-4186-a138-8f6694c537d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card Dataset (first 5 rows):\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Data preprocessing completed successfully!\n",
      "Training set shape: (226980, 30)\n",
      "Test set shape: (56746, 30)\n",
      "Training class distribution:\n",
      " Class\n",
      "0    226602\n",
      "1       378\n",
      "Name: count, dtype: int64\n",
      "Test class distribution:\n",
      " Class\n",
      "0    56651\n",
      "1       95\n",
      "Name: count, dtype: int64\n",
      "After SMOTE:\n",
      "Count of class 0: 226602\n",
      "Count of class 1: 226602\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the credit card dataset\n",
    "creditcard_df = pd.read_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\ai2\\\\kaim-week-08\\\\data\\\\Cleaned_Credit_Card_Data.csv')\n",
    "print(\"Credit Card Dataset (first 5 rows):\")\n",
    "print(creditcard_df.head())\n",
    "\n",
    "# Separate features and target ('Class')\n",
    "X_credit = creditcard_df.drop(columns=['Class'])\n",
    "y_credit = creditcard_df['Class']\n",
    "\n",
    "# Split into train and test sets (80/20 split, with stratification to maintain the class imbalance in test set)\n",
    "X_credit_train, X_credit_test, y_credit_train, y_credit_test = train_test_split(\n",
    "    X_credit, y_credit, test_size=0.2, random_state=42, stratify=y_credit\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler_credit = StandardScaler()\n",
    "X_credit_train_scaled = scaler_credit.fit_transform(X_credit_train)\n",
    "X_credit_test_scaled = scaler_credit.transform(X_credit_test)\n",
    "\n",
    "print(\"Data preprocessing completed successfully!\")\n",
    "print(\"Training set shape:\", X_credit_train_scaled.shape)\n",
    "print(\"Test set shape:\", X_credit_test_scaled.shape)\n",
    "print(\"Training class distribution:\\n\", y_credit_train.value_counts())\n",
    "print(\"Test class distribution:\\n\", y_credit_test.value_counts())\n",
    "\n",
    "# Apply SMOTE to the training data only to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_credit_train_res, y_credit_train_res = smote.fit_resample(X_credit_train_scaled, y_credit_train)\n",
    "\n",
    "# Check the new class distribution in the training set\n",
    "print(\"After SMOTE:\")\n",
    "print(\"Count of class 0:\", sum(y_credit_train_res == 0))\n",
    "print(\"Count of class 1:\", sum(y_credit_train_res == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3509590a-6901-4a48-8fbb-5da2df1caa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud Data Dataset (first 5 rows):\n",
      "   user_id          signup_time        purchase_time  purchase_value  \\\n",
      "0   247547  2015-06-28 03:00:34  2015-08-09 03:57:29              47   \n",
      "1   220737  2015-01-28 14:21:11  2015-02-11 20:28:28              15   \n",
      "2   390400  2015-03-19 20:49:09  2015-04-11 23:41:23              44   \n",
      "3    69592  2015-02-24 06:11:57  2015-05-23 16:40:14              55   \n",
      "4   174987  2015-07-07 12:58:11  2015-11-03 04:04:30              51   \n",
      "\n",
      "       device_id  source browser sex  age    ip_address  class    ip_int  \\\n",
      "0  KIXYSVCHIPQBR     SEO  Safari   F   30  1.677886e+07      0  16778864   \n",
      "1  PKYOWQKWGJNJI     SEO  Chrome   F   34  1.684205e+07      0  16842046   \n",
      "2  LVCSXLISZHVUO     Ads      IE   M   29  1.684366e+07      0  16843657   \n",
      "3  UHAUHNXXUADJE  Direct  Chrome   F   30  1.693873e+07      0  16938733   \n",
      "4  XPGPMOHIDRMGE     SEO  Chrome   F   37  1.697198e+07      0  16971984   \n",
      "\n",
      "   lower_bound_ip_address  upper_bound_ip_address    country  purchase_hour  \\\n",
      "0              16778240.0              16779263.0  Australia              3   \n",
      "1              16809984.0              16842751.0   Thailand             20   \n",
      "2              16843264.0              16843775.0      China             23   \n",
      "3              16924672.0              16941055.0      China             16   \n",
      "4              16941056.0              16973823.0   Thailand              4   \n",
      "\n",
      "   purchase_day_of_week  user_transaction_count  \n",
      "0                     6                       1  \n",
      "1                     2                       1  \n",
      "2                     5                       1  \n",
      "3                     5                       1  \n",
      "4                     1                       1  \n",
      "Data preprocessing completed successfully!\n",
      "Training set shape: (103316, 118)\n",
      "Test set shape: (25830, 118)\n",
      "Training class distribution:\n",
      " class\n",
      "0    93502\n",
      "1     9814\n",
      "Name: count, dtype: int64\n",
      "Test class distribution:\n",
      " class\n",
      "0    23376\n",
      "1     2454\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After applying SMOTE on the training data:\n",
      "Resampled training set shape: (187004, 118)\n",
      "Resampled class distribution:\n",
      "Class 0 count: 93502\n",
      "Class 1 count: 93502\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Load the fraud data dataset\n",
    "fraud_df = pd.read_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\ai2\\\\kaim-week-08\\\\data\\\\Cleaned_Fraud_Data.csv')\n",
    "print(\"\\nFraud Data Dataset (first 5 rows):\")\n",
    "print(fraud_df.head())\n",
    "\n",
    "# Convert date columns to datetime\n",
    "fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time'])\n",
    "fraud_df['purchase_time'] = pd.to_datetime(fraud_df['purchase_time'])\n",
    "\n",
    "# Create a new feature: time (in hours) from signup to purchase\n",
    "fraud_df['time_to_purchase'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).dt.total_seconds() / 3600\n",
    "\n",
    "# Drop the original date columns (or keep them if further processing is desired)\n",
    "fraud_df = fraud_df.drop(columns=['signup_time', 'purchase_time'])\n",
    "\n",
    "# Convert ip_address to numeric (if not already)\n",
    "fraud_df['ip_address'] = pd.to_numeric(fraud_df['ip_address'], errors='coerce')\n",
    "\n",
    "# Identify categorical columns that need encoding\n",
    "categorical_cols = ['device_id', 'source', 'browser', 'sex', 'country']\n",
    "\n",
    "# Function to reduce unique categories for high-cardinality columns\n",
    "def reduce_categories(df, column, threshold=50):\n",
    "    top_categories = df[column].value_counts().nlargest(threshold).index\n",
    "    df[column] = df[column].apply(lambda x: x if x in top_categories else \"Other\")\n",
    "    return df\n",
    "\n",
    "# Reduce unique values for high-cardinality categorical columns\n",
    "for col in categorical_cols:\n",
    "    fraud_df = reduce_categories(fraud_df, col, threshold=50)\n",
    "\n",
    "# Apply one-hot encoding after reducing unique categories\n",
    "fraud_df = pd.get_dummies(fraud_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Separate features and target ('class')\n",
    "X_fraud = fraud_df.drop(columns=['class'])\n",
    "y_fraud = fraud_df['class']\n",
    "\n",
    "# Split into train and test sets with stratification\n",
    "X_fraud_train, X_fraud_test, y_fraud_train, y_fraud_test = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.2, random_state=42, stratify=y_fraud\n",
    ")\n",
    "\n",
    "# Scale the fraud dataset features\n",
    "scaler_fraud = StandardScaler()\n",
    "X_fraud_train_scaled = scaler_fraud.fit_transform(X_fraud_train)\n",
    "X_fraud_test_scaled = scaler_fraud.transform(X_fraud_test)\n",
    "\n",
    "print(\"Data preprocessing completed successfully!\")\n",
    "print(\"Training set shape:\", X_fraud_train_scaled.shape)\n",
    "print(\"Test set shape:\", X_fraud_test_scaled.shape)\n",
    "print(\"Training class distribution:\\n\", y_fraud_train.value_counts())\n",
    "print(\"Test class distribution:\\n\", y_fraud_test.value_counts())\n",
    "\n",
    "# Addressing Imbalance with SMOTE\n",
    "# Apply SMOTE to the training data only\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_fraud_train_res, y_fraud_train_res = smote.fit_resample(X_fraud_train_scaled, y_fraud_train)\n",
    "\n",
    "print(\"\\nAfter applying SMOTE on the training data:\")\n",
    "print(\"Resampled training set shape:\", X_fraud_train_res.shape)\n",
    "print(\"Resampled class distribution:\")\n",
    "print(\"Class 0 count:\", sum(y_fraud_train_res == 0))\n",
    "print(\"Class 1 count:\", sum(y_fraud_train_res == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9476b361-ccf6-49f2-9df1-cffc9ba89719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training and Evaluation on Credit Card Data (with SMOTE applied) ---\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.9736721531033025\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56651\n",
      "           1       0.05      0.87      0.10        95\n",
      "\n",
      "    accuracy                           0.97     56746\n",
      "   macro avg       0.53      0.92      0.54     56746\n",
      "weighted avg       1.00      0.97      0.99     56746\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.9974095090402848\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56651\n",
      "           1       0.35      0.64      0.45        95\n",
      "\n",
      "    accuracy                           1.00     56746\n",
      "   macro avg       0.67      0.82      0.73     56746\n",
      "weighted avg       1.00      1.00      1.00     56746\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9994889507630493\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56651\n",
      "           1       0.91      0.77      0.83        95\n",
      "\n",
      "    accuracy                           1.00     56746\n",
      "   macro avg       0.96      0.88      0.92     56746\n",
      "weighted avg       1.00      1.00      1.00     56746\n",
      "\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.9887392943996053\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56651\n",
      "           1       0.11      0.84      0.20        95\n",
      "\n",
      "    accuracy                           0.99     56746\n",
      "   macro avg       0.56      0.92      0.60     56746\n",
      "weighted avg       1.00      0.99      0.99     56746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize models for the credit card dataset\n",
    "lr_credit = LogisticRegression(max_iter=1000, random_state=42)\n",
    "dt_credit = DecisionTreeClassifier(random_state=42)\n",
    "rf_credit = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_credit = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "models_credit = {\n",
    "    \"Logistic Regression\": lr_credit,\n",
    "    \"Decision Tree\": dt_credit,\n",
    "    \"Random Forest\": rf_credit,\n",
    "    \"Gradient Boosting\": gb_credit\n",
    "}\n",
    "\n",
    "print(\"\\n--- Training and Evaluation on Credit Card Data (with SMOTE applied) ---\")\n",
    "for name, model in models_credit.items():\n",
    "    # Train the model on the balanced (resampled) training set\n",
    "    model.fit(X_credit_train_res, y_credit_train_res)\n",
    "    \n",
    "    # Predict on the original (imbalanced) test set\n",
    "    y_pred = model.predict(X_credit_test_scaled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc = accuracy_score(y_credit_test, y_pred)\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_credit_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9a6c430-cdd3-4f0e-b0d2-cb3716734a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training and Evaluation onFraud Data Dataset(with SMOTE applied) ---\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.6766550522648084\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.67      0.79     23376\n",
      "           1       0.18      0.69      0.29      2454\n",
      "\n",
      "    accuracy                           0.68     25830\n",
      "   macro avg       0.57      0.68      0.54     25830\n",
      "weighted avg       0.88      0.68      0.74     25830\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.8934959349593496\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     23376\n",
      "           1       0.45      0.57      0.51      2454\n",
      "\n",
      "    accuracy                           0.89     25830\n",
      "   macro avg       0.70      0.75      0.72     25830\n",
      "weighted avg       0.91      0.89      0.90     25830\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9556329849012776\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     23376\n",
      "           1       0.98      0.55      0.70      2454\n",
      "\n",
      "    accuracy                           0.96     25830\n",
      "   macro avg       0.97      0.77      0.84     25830\n",
      "weighted avg       0.96      0.96      0.95     25830\n",
      "\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.956600851722803\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     23376\n",
      "           1       1.00      0.55      0.70      2454\n",
      "\n",
      "    accuracy                           0.96     25830\n",
      "   macro avg       0.98      0.77      0.84     25830\n",
      "weighted avg       0.96      0.96      0.95     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize models for fraud dataset\n",
    "lr_fraud = LogisticRegression(max_iter=1000, random_state=42)\n",
    "dt_fraud = DecisionTreeClassifier(random_state=42)\n",
    "rf_fraud = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_fraud = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "models_fraud = {\n",
    "    \"Logistic Regression\": lr_fraud,\n",
    "    \"Decision Tree\": dt_fraud,\n",
    "    \"Random Forest\": rf_fraud,\n",
    "    \"Gradient Boosting\": gb_fraud\n",
    "}\n",
    "\n",
    "print(\"\\n--- Training and Evaluation onFraud Data Dataset(with SMOTE applied) ---\")\n",
    "for name, model in models_fraud.items():\n",
    "    # Train the model on the SMOTE-resampled training data\n",
    "    model.fit(X_fraud_train_res, y_fraud_train_res)\n",
    "    \n",
    "    # Predict on the original (imbalanced) test set\n",
    "    y_pred = model.predict(X_fraud_test_scaled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    acc = accuracy_score(y_fraud_test, y_pred)\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_fraud_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dab88fcc-cc6c-4394-8f54-480de5f671df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\KAIM\\kaim-week-02\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP Credit Card Model Accuracy: 0.9991365075111389\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def create_mlp_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train the MLP for the credit card dataset using SMOTE-resampled training data\n",
    "mlp_credit = create_mlp_model(X_credit_train_res.shape[1])\n",
    "history_mlp = mlp_credit.fit(X_credit_train_res, y_credit_train_res,\n",
    "                             epochs=20, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate the model on the original (imbalanced) test set\n",
    "score_mlp = mlp_credit.evaluate(X_credit_test_scaled, y_credit_test, verbose=0)\n",
    "print(\"\\nMLP Credit Card Model Accuracy:\", score_mlp[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89dcebac-3009-47bb-983e-69f5ba5f686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\KAIM\\kaim-week-02\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Credit Card Model Accuracy: 0.9991893768310547\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Reshape the SMOTE-resampled training data for CNN: (samples, timesteps, channels)\n",
    "X_credit_train_res_cnn = X_credit_train_res.reshape(-1, X_credit_train_res.shape[1], 1)\n",
    "\n",
    "# Reshape the test data as before\n",
    "X_credit_test_cnn = X_credit_test_scaled.reshape(-1, X_credit_test_scaled.shape[1], 1)\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the CNN model using the input shape from the SMOTE training data\n",
    "cnn_credit = create_cnn_model((X_credit_train_res_cnn.shape[1], 1))\n",
    "\n",
    "# Train the CNN model on the SMOTE-resampled training set\n",
    "history_cnn = cnn_credit.fit(X_credit_train_res_cnn, y_credit_train_res,\n",
    "                             epochs=20, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate the model on the original (imbalanced) test set\n",
    "score_cnn = cnn_credit.evaluate(X_credit_test_cnn, y_credit_test, verbose=0)\n",
    "print(\"CNN Credit Card Model Accuracy:\", score_cnn[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25abe2f-7a15-4b8d-af1c-b8cdf3c6b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\OneDrive\\Desktop\\KAIM\\kaim-week-02\\venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def create_rnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        SimpleRNN(50, input_shape=input_shape),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Reshape the SMOTE-resampled training data for RNN: (samples, timesteps, channels)\n",
    "X_credit_train_res_rnn = X_credit_train_res.reshape(-1, X_credit_train_res.shape[1], 1)\n",
    "\n",
    "# Reshape the test data as before\n",
    "X_credit_test_rnn = X_credit_test_scaled.reshape(-1, X_credit_test_scaled.shape[1], 1)\n",
    "\n",
    "# Create the RNN model using the input shape from the SMOTE-resampled training data\n",
    "rnn_credit = create_rnn_model((X_credit_train_res_rnn.shape[1], 1))\n",
    "\n",
    "# Train the RNN model on the SMOTE-resampled training data\n",
    "history_rnn = rnn_credit.fit(X_credit_train_res_rnn, y_credit_train_res,\n",
    "                             epochs=20, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate the model on the original (imbalanced) test set\n",
    "score_rnn = rnn_credit.evaluate(X_credit_test_rnn, y_credit_test, verbose=0)\n",
    "print(\"RNN Credit Card Model Accuracy:\", score_rnn[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ddec7-95cf-4b3e-b56a-4fa590cf90dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
