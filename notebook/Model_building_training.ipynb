{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40af52c2-0efd-4186-a138-8f6694c537d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card Dataset (first 5 rows):\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "Data preprocessing completed successfully!\n",
      "Training set shape: (226980, 30)\n",
      "Test set shape: (56746, 30)\n",
      "Training class distribution:\n",
      " Class\n",
      "0    226602\n",
      "1       378\n",
      "Name: count, dtype: int64\n",
      "Test class distribution:\n",
      " Class\n",
      "0    56651\n",
      "1       95\n",
      "Name: count, dtype: int64\n",
      "After SMOTE:\n",
      "Count of class 0: 226602\n",
      "Count of class 1: 226602\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the credit card dataset\n",
    "creditcard_df = pd.read_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\ai2\\\\kaim-week-08\\\\data\\\\Cleaned_Credit_Card_Data.csv')\n",
    "print(\"Credit Card Dataset (first 5 rows):\")\n",
    "print(creditcard_df.head())\n",
    "\n",
    "# Separate features and target ('Class')\n",
    "X_credit = creditcard_df.drop(columns=['Class'])\n",
    "y_credit = creditcard_df['Class']\n",
    "\n",
    "# Split into train and test sets (80/20 split, with stratification to maintain the class imbalance in test set)\n",
    "X_credit_train, X_credit_test, y_credit_train, y_credit_test = train_test_split(\n",
    "    X_credit, y_credit, test_size=0.2, random_state=42, stratify=y_credit\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler_credit = StandardScaler()\n",
    "X_credit_train_scaled = scaler_credit.fit_transform(X_credit_train)\n",
    "X_credit_test_scaled = scaler_credit.transform(X_credit_test)\n",
    "\n",
    "print(\"Data preprocessing completed successfully!\")\n",
    "print(\"Training set shape:\", X_credit_train_scaled.shape)\n",
    "print(\"Test set shape:\", X_credit_test_scaled.shape)\n",
    "print(\"Training class distribution:\\n\", y_credit_train.value_counts())\n",
    "print(\"Test class distribution:\\n\", y_credit_test.value_counts())\n",
    "\n",
    "# Apply SMOTE to the training data only to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_credit_train_res, y_credit_train_res = smote.fit_resample(X_credit_train_scaled, y_credit_train)\n",
    "\n",
    "# Check the new class distribution in the training set\n",
    "print(\"After SMOTE:\")\n",
    "print(\"Count of class 0:\", sum(y_credit_train_res == 0))\n",
    "print(\"Count of class 1:\", sum(y_credit_train_res == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3509590a-6901-4a48-8fbb-5da2df1caa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud Data Dataset (first 5 rows):\n",
      "   user_id          signup_time        purchase_time  purchase_value  \\\n",
      "0   247547  2015-06-28 03:00:34  2015-08-09 03:57:29              47   \n",
      "1   220737  2015-01-28 14:21:11  2015-02-11 20:28:28              15   \n",
      "2   390400  2015-03-19 20:49:09  2015-04-11 23:41:23              44   \n",
      "3    69592  2015-02-24 06:11:57  2015-05-23 16:40:14              55   \n",
      "4   174987  2015-07-07 12:58:11  2015-11-03 04:04:30              51   \n",
      "\n",
      "       device_id  source browser sex  age    ip_address  class    ip_int  \\\n",
      "0  KIXYSVCHIPQBR     SEO  Safari   F   30  1.677886e+07      0  16778864   \n",
      "1  PKYOWQKWGJNJI     SEO  Chrome   F   34  1.684205e+07      0  16842046   \n",
      "2  LVCSXLISZHVUO     Ads      IE   M   29  1.684366e+07      0  16843657   \n",
      "3  UHAUHNXXUADJE  Direct  Chrome   F   30  1.693873e+07      0  16938733   \n",
      "4  XPGPMOHIDRMGE     SEO  Chrome   F   37  1.697198e+07      0  16971984   \n",
      "\n",
      "   lower_bound_ip_address  upper_bound_ip_address    country  purchase_hour  \\\n",
      "0              16778240.0              16779263.0  Australia              3   \n",
      "1              16809984.0              16842751.0   Thailand             20   \n",
      "2              16843264.0              16843775.0      China             23   \n",
      "3              16924672.0              16941055.0      China             16   \n",
      "4              16941056.0              16973823.0   Thailand              4   \n",
      "\n",
      "   purchase_day_of_week  user_transaction_count  \n",
      "0                     6                       1  \n",
      "1                     2                       1  \n",
      "2                     5                       1  \n",
      "3                     5                       1  \n",
      "4                     1                       1  \n",
      "Data preprocessing completed successfully!\n",
      "Training set shape: (103316, 118)\n",
      "Test set shape: (25830, 118)\n",
      "Training class distribution:\n",
      " class\n",
      "0    93502\n",
      "1     9814\n",
      "Name: count, dtype: int64\n",
      "Test class distribution:\n",
      " class\n",
      "0    23376\n",
      "1     2454\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After applying SMOTE on the training data:\n",
      "Resampled training set shape: (187004, 118)\n",
      "Resampled class distribution:\n",
      "Class 0 count: 93502\n",
      "Class 1 count: 93502\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Load the fraud data dataset\n",
    "fraud_df = pd.read_csv('C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\ai2\\\\kaim-week-08\\\\data\\\\Cleaned_Fraud_Data.csv')\n",
    "print(\"\\nFraud Data Dataset (first 5 rows):\")\n",
    "print(fraud_df.head())\n",
    "\n",
    "# Convert date columns to datetime\n",
    "fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time'])\n",
    "fraud_df['purchase_time'] = pd.to_datetime(fraud_df['purchase_time'])\n",
    "\n",
    "# Create a new feature: time (in hours) from signup to purchase\n",
    "fraud_df['time_to_purchase'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).dt.total_seconds() / 3600\n",
    "\n",
    "# Drop the original date columns (or keep them if further processing is desired)\n",
    "fraud_df = fraud_df.drop(columns=['signup_time', 'purchase_time'])\n",
    "\n",
    "# Convert ip_address to numeric (if not already)\n",
    "fraud_df['ip_address'] = pd.to_numeric(fraud_df['ip_address'], errors='coerce')\n",
    "\n",
    "# Identify categorical columns that need encoding\n",
    "categorical_cols = ['device_id', 'source', 'browser', 'sex', 'country']\n",
    "\n",
    "# Function to reduce unique categories for high-cardinality columns\n",
    "def reduce_categories(df, column, threshold=50):\n",
    "    top_categories = df[column].value_counts().nlargest(threshold).index\n",
    "    df[column] = df[column].apply(lambda x: x if x in top_categories else \"Other\")\n",
    "    return df\n",
    "\n",
    "# Reduce unique values for high-cardinality categorical columns\n",
    "for col in categorical_cols:\n",
    "    fraud_df = reduce_categories(fraud_df, col, threshold=50)\n",
    "\n",
    "# Apply one-hot encoding after reducing unique categories\n",
    "fraud_df = pd.get_dummies(fraud_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Separate features and target ('class')\n",
    "X_fraud = fraud_df.drop(columns=['class'])\n",
    "y_fraud = fraud_df['class']\n",
    "\n",
    "# Split into train and test sets with stratification\n",
    "X_fraud_train, X_fraud_test, y_fraud_train, y_fraud_test = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.2, random_state=42, stratify=y_fraud\n",
    ")\n",
    "\n",
    "# Scale the fraud dataset features\n",
    "scaler_fraud = StandardScaler()\n",
    "X_fraud_train_scaled = scaler_fraud.fit_transform(X_fraud_train)\n",
    "X_fraud_test_scaled = scaler_fraud.transform(X_fraud_test)\n",
    "\n",
    "print(\"Data preprocessing completed successfully!\")\n",
    "print(\"Training set shape:\", X_fraud_train_scaled.shape)\n",
    "print(\"Test set shape:\", X_fraud_test_scaled.shape)\n",
    "print(\"Training class distribution:\\n\", y_fraud_train.value_counts())\n",
    "print(\"Test class distribution:\\n\", y_fraud_test.value_counts())\n",
    "\n",
    "# Addressing Imbalance with SMOTE\n",
    "# Apply SMOTE to the training data only\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_fraud_train_res, y_fraud_train_res = smote.fit_resample(X_fraud_train_scaled, y_fraud_train)\n",
    "\n",
    "print(\"\\nAfter applying SMOTE on the training data:\")\n",
    "print(\"Resampled training set shape:\", X_fraud_train_res.shape)\n",
    "print(\"Resampled class distribution:\")\n",
    "print(\"Class 0 count:\", sum(y_fraud_train_res == 0))\n",
    "print(\"Class 1 count:\", sum(y_fraud_train_res == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476b361-ccf6-49f2-9df1-cffc9ba89719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
